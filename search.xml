<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[聊聊环境隔离]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%81%8A%E8%81%8A%E7%8E%AF%E5%A2%83%E9%9A%94%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[环境问题是日常开发以及服务上线碰到的问题。这里简单聊下自己所知道的环境隔离的方案 参考有赞环境隔离分享]]></content>
      <tags>
        <tag>环境隔离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊dubbo与zookeeper]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%81%8A%E8%81%8Adubbo%E4%B8%8Ezookeeper%2F</url>
    <content type="text"><![CDATA[dubbo简单来讲就是一个远程调用RPC协议框架。本质上还是个协议 zookeeper是一个注册中心。本质上还是一个软件(software) dubbo 分组 vs zk分组dubbo 分组本意上是当应用有多种版本的时候，可以用group。 （从这点来看用来做环境弱隔离也是可以的）zk分组实际上是根据物理机ip分组。]]></content>
      <tags>
        <tag>java dubbo zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy(1)深入-环境配置]]></title>
    <url>%2F2019%2F07%2F02%2Fscrapy-1%2F</url>
    <content type="text"><![CDATA[scrapy 是python 爬虫框架中用度最广，且简单的流行款。这里笔者依据源码，分析下 scrapy crawl xxx 之后的操作。 scrapy crawl xxx 调用流程 由setup.py 123entry_points=&#123; &apos;console_scripts&apos;: [&apos;scrapy = scrapy.cmdline:execute&apos;] &#125; 我们找到开始的python 脚本 scrapy.cmdline:execute 主要做了下面几件事情 初始化环境配置，在项目中运行项目 初始化CrawlProcess 调用run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def execute(argv=None, settings=None): if argv is None: argv = sys.argv # --- 兼容之前版本配置 --- if settings is None and &apos;scrapy.conf&apos; in sys.modules: from scrapy import conf if hasattr(conf, &apos;settings&apos;): settings = conf.settings # ------------------------------------------------------------------ # 初始化scrapy.cfg 以及 scrapy.settings.default_setting.py 中的配置 if settings is None: settings = get_project_settings() # set EDITOR from environment if available try: editor = os.environ[&apos;EDITOR&apos;] except KeyError: pass else: settings[&apos;EDITOR&apos;] = editor # 输出deprecated 配置 check_deprecated_settings(settings) # --- 兼容之前版本配置--- import warnings from scrapy.exceptions import ScrapyDeprecationWarning with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;, ScrapyDeprecationWarning) from scrapy import conf conf.settings = settings # ------------------------------------------------------------------ # 判断是否在project中 inproject = inside_project() # 加载支持的命令 即package scrapy.commands下面的所有的文件 集合成字典 cmds = _get_commands_dict(settings, inproject) cmdname = _pop_command_name(argv) parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \ conflict_handler=&apos;resolve&apos;) if not cmdname: _print_commands(settings, inproject) sys.exit(0) elif cmdname not in cmds: _print_unknown_command(settings, cmdname, inproject) sys.exit(2) cmd = cmds[cmdname] parser.usage = &quot;scrapy %s %s&quot; % (cmdname, cmd.syntax()) parser.description = cmd.long_desc() settings.setdict(cmd.default_settings, priority=&apos;command&apos;) cmd.settings = settings cmd.add_options(parser) # 解析参数 opts, args = parser.parse_args(args=argv[1:]) # 这里是对命令中一些参数进行设置 比如是 ```-logfile=xxx.xx```等 _run_print_help(parser, cmd.process_options, args, opts) # 爬虫运行类 cmd.crawler_process = CrawlerProcess(settings) # 执行 cmd 中的run类。 ** 在这里就是执行scrapy.commands.crawler.run函数** _run_print_help(parser, _run_command, cmd, args, opts) sys.exit(cmd.exitcode) scray.crawler.CrawlerProcess 这个类是继承了scray.crawler.CrawlerRunner，主要是增加了一个start函数。需要注意的是init 函数。 scray.crawler.CrawlerRunner.init 中有个属性是spider_loader 是所有爬虫类的加载器，默认使用的是 default_setting.py中的 SPIDER_LOADER_CLASS =’scrapy.spiderloader.SpiderLoader’。 scrapy.commands.crawler.run 这里就是调用了CrawlProcess.crawl/start 函数 123456789101112def run(self, args, opts): if len(args) &lt; 1: raise UsageError() elif len(args) &gt; 1: raise UsageError(&quot;running &apos;scrapy crawl&apos; with more than one spider is no longer supported&quot;) spname = args[0] self.crawler_process.crawl(spname, **opts.spargs) self.crawler_process.start() if self.crawler_process.bootstrap_failed: self.exitcode = 1 CrawlProcess.crawl/CrawlerRunner.crawl 实际上这个调用的是父类 rawlerRunner.crawl 主要工作 初始化并返回一个scrapy.crawler.Crawler实例 调用_crawl函数12345678910def crawl(self, crawler_or_spidercls, *args, **kwargs): if isinstance(crawler_or_spidercls, Spider): raise ValueError( &apos;The crawler_or_spidercls argument cannot be a spider object, &apos; &apos;it must be a spider class (or a Crawler object)&apos;) ## 返回一个scrapy.crawler.Crawler对象 crawler = self.create_crawler(crawler_or_spidercls) ## 调用 CrawlerRunner._crawl 函数 return self._crawl(crawler, *args, **kwargs) scrapy.crawler.Crawler 初始化1234567891011121314151617181920212223242526272829303132333435363738def __init__(self, spidercls, settings=None): if isinstance(spidercls, Spider): raise ValueError( &apos;The spidercls argument must be a class, not an object&apos;) if isinstance(settings, dict) or settings is None: settings = Settings(settings) self.spidercls = spidercls self.settings = settings.copy() self.spidercls.update_settings(self.settings) d = dict(overridden_settings(self.settings)) logger.info(&quot;Overridden settings: %(settings)r&quot;, &#123;&apos;settings&apos;: d&#125;) self.signals = SignalManager(self) self.stats = load_object(self.settings[&apos;STATS_CLASS&apos;])(self) handler = LogCounterHandler(self, level=self.settings.get(&apos;LOG_LEVEL&apos;)) logging.root.addHandler(handler) if get_scrapy_root_handler() is not None: # scrapy root handler already installed: update it with new settings install_scrapy_root_handler(self.settings) # lambda is assigned to Crawler attribute because this way it is not # garbage collected after leaving __init__ scope self.__remove_handler = lambda: logging.root.removeHandler(handler) self.signals.connect(self.__remove_handler, signals.engine_stopped) lf_cls = load_object(self.settings[&apos;LOG_FORMATTER&apos;]) self.logformatter = lf_cls.from_crawler(self) self.extensions = ExtensionManager.from_crawler(self) self.settings.freeze() self.crawling = False # spider 实例 初始化 self.spider = None # 核心engine self.engine = NoneCrawlerRunner._crawl 主要工作 上面初始化了一个Crawler 对象， 调用Crawler.crawl函数（初始化引擎等其他配置） 12345678910111213def _crawl(self, crawler, *args, **kwargs): self.crawlers.add(crawler) // Crawler.crawl d = crawler.crawl(*args, **kwargs) self._active.add(d) def _done(result): self.crawlers.discard(crawler) self._active.discard(d) self.bootstrap_failed |= not getattr(crawler, &apos;spider&apos;, None) return result return d.addBoth(_done) scapy.crawler.Crawler.crawl 主要工作 实例化spider （这个源码就跳过了，就是简单的创建spider实例，并将参数设置好） 实例化engine (下面详细讲下)12345678910111213141516171819202122232425262728293031@defer.inlineCallbacksdef crawl(self, *args, **kwargs): assert not self.crawling, &quot;Crawling already taking place&quot; self.crawling = True try: self.spider = self._create_spider(*args, **kwargs) ## 这个就是实例化了一个scrapy.engine.ExecutionEngine类 self.engine = self._create_engine() start_requests = iter(self.spider.start_requests()) yield self.engine.open_spider(self.spider, start_requests) yield defer.maybeDeferred(self.engine.start) except Exception: # In Python 2 reraising an exception after yield discards # the original traceback (see https://bugs.python.org/issue7563), # so sys.exc_info() workaround is used. # This workaround also works in Python 3, but it is not needed, # and it is slower, so in Python 3 we use native `raise`. if six.PY2: exc_info = sys.exc_info() self.crawling = False if self.engine is not None: yield self.engine.close() if six.PY2: six.reraise(*exc_info) raisedef _create_engine(self): return ExecutionEngine(self, lambda _: self.stop()) scrapy.engine.ExecutionEngine init 函数主要工作 加载调度器(默认是SCHEDULER = ‘scrapy.core.scheduler.Scheduler’) 加载下载器(默认是DOWNLOADER = ‘scrapy.core.downloader.Downloader’) 初始化 scrapy.core.scrapyer.Scraper 对象(下面详解)1234567891011121314151617class ExecutionEngine(object): def __init__(self, crawler, spider_closed_callback): self.crawler = crawler self.settings = crawler.settings self.signals = crawler.signals self.logformatter = crawler.logformatter self.slot = None self.spider = None self.running = False self.paused = False self.scheduler_cls = load_object(self.settings[&apos;SCHEDULER&apos;]) downloader_cls = load_object(self.settings[&apos;DOWNLOADER&apos;]) self.downloader = downloader_cls(crawler) # self.scraper = Scraper(crawler) self._spider_closed_callback = spider_closed_callback scrapy.core.scrapyer.Scraper 主要工作 (连接 middleware pipline spider) 初始化中间件 初始化item1234567891011class Scraper(object): def __init__(self, crawler): self.slot = None self.spidermw = SpiderMiddlewareManager.from_crawler(crawler) itemproc_cls = load_object(crawler.settings[&apos;ITEM_PROCESSOR&apos;]) self.itemproc = itemproc_cls.from_crawler(crawler) self.concurrent_items = crawler.settings.getint(&apos;CONCURRENT_ITEMS&apos;) self.crawler = crawler self.signals = crawler.signals self.logformatter = crawler.logformatter 到此初始化就完成了。]]></content>
      <tags>
        <tag>scrapy 爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之Flink 简单介绍]]></title>
    <url>%2F2019%2F06%2F15%2FFlink-4-What-is-Flink%2F</url>
    <content type="text"><![CDATA[通过前几篇Flink 实战的文章，应该对Flink有点印象了。接下来，本片文章就简单从基本概念，场景， 架构， 特点等方面介绍下Flink. 什么是Flink 官网介绍如下Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. 个人理解Flink 是一个框架和分布式处理引擎，可以对无界或有界数据流进行状态计算。 注意加粗的几个字是核心。下面会聊到。 能够解决什么问题 通用处理流/批处理。 适用场景 实时智能推荐 复杂事件处理 实时数仓与ETL 流数据分析 实时报表分析 基本的概念 流/批处理处理思想 Flink对于批处理认为是有界的流处理。而Spark则认为流处理是更快的批处理。那个更有优势不重要，符合自己的需求并扩展性良好即可。 流 stream 有界流（批处理）vs 无界流（流处理） 有界流：一批连续的数据有开始有结束。（几何中的段的概念） 无界流：连续且无边界的数据流。（几何中射线的概念） 实时流和record Stream 实时流：举例天猫双十一大盘 record Stream： 离线数据清洗。 状态 state 状态可以简单理解为在处理数据的过程中，每个数据的改变都是状态的改变。 时间 time Event time 时间发生的时间 Process Time 消息被计算处理的时间 Ingestion Time 摄取时间：事件进入流处理系统的时间。 架构 组件结构 参考 API&amp;&amp;组件库层 Flink 同时提供了高级的流处理API(eg flatmap filter等),以及相对低级的processFunction. 在API的基础上，抽象除了不同场景下的组件库（FlinkML(机器学习)，CEP(复杂事件处理)等） Runtime层 是Flink的核心层。支持分布式作业的执行，任务调度等。 Deploy 层 部署相关。支持 local, yarn ,cloud等运行环境。 注意 FLink组件库调用API(StreamAPI或者是DataSetAPI)， API(StreamAPI或者是DataSetAPI) 生成jobGraph,并传递给Runtime层，jobGraph 根据不同的部署环境，采用不同的配置项（Flink内置的）执行。 Flink 集群运行时架构 参考 jobManagers(master) 负责 （至少有一个） 协调分布式执行 任务调度 协调检查 协调错误恢复 taskManagers(slave) 负责 （至少有一个） 执行数据流任务 缓存并交换流数据 client 作为数据源的输入，输入之后，可以被清除，也可以或者是处理其他的任务。 特性 流处理 支持高吞吐，低延迟，高性能流处理操作 支持高度灵活窗口(slideWindow SessionWindow等) 支持状态计算，同时具有exactly-once特性 支持 batch on stream API StreamAPI BatchAPI 众多Libraries 机器学习 图处理 。。。 总结Flink 本质上还是只是一个专门解决流批数据处理的框架，并且在性能，稳定，开发，部署等方面具有独到之处。如果我们日常需求中有涉及到大数据处理，且很可能会涉及到协同分布式等，Flink是一个很好的选择。 参考Flink 官网文档]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之收入最高出租车司机]]></title>
    <url>%2F2019%2F06%2F14%2FFlink-learning-in-action-3%2F</url>
    <content type="text"><![CDATA[本篇文章使用纽约市2009-1015年关于出租车驾驶的公共数据集，模拟现实数据流，获取一定时间内收入最高的出租车司机。 输入输出输入: 详见下面数据集输出：每个小时收入收入topN的driverId.额外条件：模拟丢失数据。每n条记录丢失一条数据。 数据集网站New York City Taxi &amp; Limousine Commission提供了关于纽约市从2009-1015年关于出租车驾驶的公共数据集。 下载：12wget http://training.ververica.com/trainingData/nycTaxiRides.gzwget http://training.ververica.com/trainingData/nycTaxiFares.gz TaxiRides 行程信息。每次出行包含两条记录。分别标识为 行程开始start 和 行程结束end。数据集结构 1234567891011rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一id driverId : Long // 出租车司机唯一idisStart : Boolean // 是否是行程开始。false标识行程结束 startTime : DateTime // 行程开始时间endTime : DateTime // 行程结束时间 对于行程开始记录该值为 1970-01-01 00:00:00startLon : Float // 开始行程的经度startLat : Float // 开始行程的纬度endLon : Float // 结束的经度endLat : Float // 结束的纬度passengerCnt : Short // 乘客数量 TaxiRides 数据示例 TaxiFares 费用信息。 与上面行程信息对应 12345678rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一iddriverId : Long // 出租车司机唯一idstartTime : DateTime // 行程开始时间paymentType : String // CSH or CRDtip : Float // tip for this ride (消费)tolls : Float // tolls for this ridetotalFare : Float // total fare collected TaxiFares 数据示例 分析通过上面的数据集以及输入输出的要求，可以分析如下：先根据上满两个数据集，生成输入流。再根据ridrId进行join，对join的结果进行窗口分割，最后对窗口内的数据入库计算收入最高的n个driverId. 思路 生成数据流。（读取上面两个数据集） 模拟丢失数据 filter 根据routId 将两个输入流join (这里其实是过滤掉了filter过滤掉的数据的对应数据) 对上面join的结果划分窗口，并以driverId分组计算窗口内收入，(这里就是简单对taxiFare进行取topN) 选出topN 输出。 部分核心实现新建两个class 表示ride和fare 完整代码 对应source 完整代码 主要逻辑 完整代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public static void main(String[] args) throws Exception &#123; // 初始化enviorment StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置事件事件 environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 读取输入流 DataStreamSource&lt;TaxiFare&gt; taxiFareDataStreamSource = environment .addSource(new TaxiFareSource()); DataStream&lt;TaxiFare&gt; taxiFare = taxiFareDataStreamSource .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getRideId(); &#125; &#125;);// taxiFareDataStreamSource.print(); DataStreamSource&lt;TaxiRide&gt; taxiRideDataStreamSource = environment .addSource(new TaxiRideSource());// taxiRideDataStreamSource.print(); // 对source 过滤掉rided % 1000 == 0, 模拟现实世界丢失数据 DataStream&lt;TaxiRide&gt; taxiRide = taxiRideDataStreamSource.filter(new FilterFunction&lt;TaxiRide&gt;() &#123; @Override public boolean filter(TaxiRide value) throws Exception &#123; return value.isStart &amp;&amp; value.getRideId() % 1000 != 0; &#125; &#125;) .keyBy(new KeySelector&lt;TaxiRide, Long&gt;() &#123; @Override public Long getKey(TaxiRide value) throws Exception &#123; return value.getRideId(); &#125; &#125;); // join 将两个输入流以rideId为key， 合并 SingleOutputStreamOperator&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;&gt; process = taxiFare.connect(taxiRide) .process(new ConnectProcess()); // 设置窗口 SingleOutputStreamOperator&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; aggregate = process // 先将taxiFare 筛选出来，因为是要统计topN taxiFare .flatMap(new FlatMapFunction&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;, TaxiFare&gt;() &#123; @Override public void flatMap(Tuple2&lt;TaxiFare, TaxiRide&gt; value, Collector&lt;TaxiFare&gt; out) throws Exception &#123; out.collect(value.f0); &#125; &#125;) // 因为是时间递增，所以watermark 很简单 .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;TaxiFare&gt;() &#123; @Override public long extractAscendingTimestamp(TaxiFare element) &#123;// System.out.println(element.getEventTime()); return element.getEventTime(); &#125; &#125;) // 根据 driverId 分组 .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getDriverId(); &#125; &#125;) // 设置时间窗口，每30min 计算一次最近1个小时的内driverId的总收入 .timeWindow(Time.hours(1), Time.minutes(30)) // 这个是累加函数,调用aggregate 结果会计算出同一个窗口中，每个driverId的收入总值 .aggregate(getAggregateFunction(), // 这个windowFunc 是格式化输出 new WindowFunction&lt;Float, Tuple3&lt;Long, Float, Timestamp&gt;, Long, TimeWindow&gt;() &#123; @Override public void apply(Long driverId, TimeWindow window, Iterable&lt;Float&gt; input, Collector&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; out) throws Exception &#123; Float next = input.iterator().next(); out.collect(new Tuple3(driverId, next, new Timestamp(window.getEnd()))); &#125; &#125;); // topSize N int topSize = 3; aggregate // 根据时间进行分窗口 .keyBy(2) .timeWindow(Time.hours(1), Time.minutes(30)) .process(new topN(topSize)).print(); environment.execute("RideAndFareExercise"); &#125; 运行结果 参考[ververica]https://training.ververica.com/]]></content>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之获取topNWord]]></title>
    <url>%2F2019%2F06%2F12%2FFlink-learning-in-action-2%2F</url>
    <content type="text"><![CDATA[本片文章基于上一篇文章的逻辑上增加获取topN的逻辑，可以加深对Flink的认识。 描述上一篇文章中只是统计并输出了一定时间内的相同单词的次数，这次我们更深入点，统计一定时间内的前N个word. 思路： 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 增加新窗口（里面的数据是上一步统计好次数之后的单词） 对上面窗口中的数据排序，输出前topN print 输出。 部分代码123456789101112// 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; ret = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 相同字母次数相加 .reduce(CountReduce()) // 滑动窗口，每 30s为一个窗口，每5s计算一次 （新增的逻辑） .windowAll(SlidingProcessingTimeWindows.of(Time.seconds(30), Time.seconds(5))) // 对同一个窗口的所有元素排序取前topSize （新增的逻辑） .process(new TopN(topSize)); TopN class 12345678910111213141516171819202122232425262728293031323334353637383940414243static class TopN extends ProcessAllWindowFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, TimeWindow&gt; &#123; private final int topSize; TopN(int topSize) &#123; this.topSize = topSize; &#125; @Override public void process(Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; elements, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; /* 1 先创建一颗有序树， 2 依次往树里面放数据 3 如果超过topSize 那么就去掉树的最后一个节点 */ TreeMap&lt;Long, Tuple2&lt;String, Long&gt;&gt; treeMap = new TreeMap&lt;&gt;( new Comparator&lt;Long&gt;() &#123; @Override public int compare(Long o1, Long o2) &#123; return o2 &gt; o1 ? 1 : -1; &#125; &#125; ); for (Tuple2&lt;String, Long&gt; element : elements) &#123; treeMap.put(element.f1, element); if (treeMap.size() &gt; this.topSize) &#123; treeMap.pollLastEntry(); &#125; &#125; for (Entry&lt;Long, Tuple2&lt;String, Long&gt;&gt; longTuple2Entry : treeMap.entrySet()) &#123; out.collect(longTuple2Entry.getValue()); &#125; &#125; &#125; 完整代码完整代码请点我]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之第一个Flink程序]]></title>
    <url>%2F2019%2F06%2F11%2FFlink-learning-in-action-1%2F</url>
    <content type="text"><![CDATA[通过本篇文章，帮助你通过使用Maven 快速实现官网demo. 环境 操作系统： mac java版本：1.8 Flink版本：1.7.2 scala版本：2.11 maven 版本:Apache Maven 3.5.2 描述输入为连续的单词，每5s对30s内的单词进行计数并输出。 使用Maven 创建项目12345678mvn archetype:generate \ -DarchetypeGroupId=org.apache.Flink \ -DarchetypeArtifactId=Flink-quickstart-java \ -DarchetypeVersion=1.7.2 \ -DgroupId=Flink-learning-in-action \ -DartifactId=Flink-learning-in-action \ -Dversion=0.1 \ -Dpackage=myFlink 运行成功之后会再当前目录下生成一个名为 Flink-learning-in-action的目录。目录结构 1234567891011$ tree Flink-learning-in-actionFlink-learning-in-action├── pom.xml└── src └── main ├── java │ └── myFlink │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 代码思想 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 print 输出。 部分代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class SocketWindowCount &#123; public static void main(String[] args) throws Exception &#123; ParameterTool parameterTool = ParameterTool.fromArgs(args); String host = parameterTool.get("host", "localhost"); int port = parameterTool.getInt("port", 9000); // 先初始化执行环境 StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并发度为1， 方便观察输出 environment.setParallelism(1); // 输入流 DataStreamSource&lt;String&gt; socketTextStream = environment.socketTextStream(host, port); // 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; reduce = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 计算 .reduce(CountReduce()); // 打印到控制台 输出时间 reduce.addSink(new RichSinkFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void invoke(Tuple2&lt;String, Long&gt; value, Context context) &#123; System.out.println(now() + " word: " + value.f0 + " count: " + value.f1); &#125; &#125;); environment.execute("SocketWindowCount"); &#125; // 统计相同值出现的次数 private static ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; CountReduce() &#123; return new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123; return new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1); &#125; &#125;; &#125; // 将输入的一行 分割成单词，并初始化次数为1 private static FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt; split2Word() &#123; return new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\\W"); for (String word : words) &#123; if (word.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125; &#125; &#125;; &#125;&#125; 运行1 打开终端执行 1nc -l 9000 2 运行代码。 结果: 完整代码完整代码 参考资料java_api_quickstart]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
