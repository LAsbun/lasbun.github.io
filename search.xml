<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[scrapy(2)之执行分析]]></title>
    <url>%2F2019%2F07%2F17%2Fscrapy-2%2F</url>
    <content type="text"><![CDATA[根据之前写的scrapy-1,我们分析到了初始化引擎。下面就实际运行是如何运行的。 scapy.crawler.Crawler.crawl 主要工作 实例化spider （这个源码就跳过了，就是简单的创建spider实例，并将参数设置好） 实例化engine (下面详细讲下)1234567891011121314151617181920212223242526272829303132@defer.inlineCallbacksdef crawl(self, *args, **kwargs): assert not self.crawling, &quot;Crawling already taking place&quot; self.crawling = True try: self.spider = self._create_spider(*args, **kwargs) ## 这个就是实例化了一个scrapy.core.engine.ExecutionEngine类 self.engine = self._create_engine() # 这里就是从自定义的start_url里面读取url,并封装成 scrapy.http.request.Request实例 start_requests = iter(self.spider.start_requests()) yield self.engine.open_spider(self.spider, start_requests) yield defer.maybeDeferred(self.engine.start) except Exception: # In Python 2 reraising an exception after yield discards # the original traceback (see https://bugs.python.org/issue7563), # so sys.exc_info() workaround is used. # This workaround also works in Python 3, but it is not needed, # and it is slower, so in Python 3 we use native `raise`. if six.PY2: exc_info = sys.exc_info() self.crawling = False if self.engine is not None: yield self.engine.close() if six.PY2: six.reraise(*exc_info) raisedef _create_engine(self): return ExecutionEngine(self, lambda _: self.stop()) 这个函数在creat_engine之后，调用了open_spider函数。 scrapy.core.engine.ExecutionEngine.open_spider 主要做了以下动作 将_next_request 注册到twisted 实例化调度器 中间件封装start_request 将事件，start_reqeusts,调度器封装成slot 并将其设置为实例属性 调用scheduler.open函数，初始化去重过滤器中间件（默认是DUPEFILTER_CLASS = ‘scrapy.dupefilters.RFPDupeFilter’） 调用scraper.open_spider(spider) 生成pipline slot.nextcall.schedule() 这个是调用前面注册成CallLaterOnce的_next_request函数。 123456789101112131415161718192021@defer.inlineCallbacks def open_spider(self, spider, start_requests=(), close_if_idle=True): assert self.has_capacity(), &quot;No free spider slot when opening %r&quot; % \ spider.name logger.info(&quot;Spider opened&quot;, extra=&#123;&apos;spider&apos;: spider&#125;) ## 这里是将_next_requests 注册到了twisted nextcall = CallLaterOnce(self._next_request, spider) scheduler = self.scheduler_cls.from_crawler(self.crawler) start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider) slot = Slot(start_requests, close_if_idle, nextcall, scheduler) self.slot = slot self.spider = spider # 初始化去重过滤器中间件（默认是DUPEFILTER_CLASS = &apos;scrapy.dupefilters.RFPDupeFilter&apos;） yield scheduler.open(spider) # 生成pipline yield self.scraper.open_spider(spider) self.crawler.stats.open_spider(spider) yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider) # 这个是调用前面注册成CallLaterOnce的_next_request函数。 slot.nextcall.schedule() slot.heartbeat.start(5) _next_reuqests 需要注意的是 在调用 _next_request_from_scheduler 的时候，调用了Download 第一次调用_next_request_from_scheduler是没有的数据的，调用self._crawl之后才会吧request放进队列里面12345678910111213141516171819202122232425262728def _next_request(self, spider): slot = self.slot if not slot: return if self.paused: return # 是否需要等待 while not self._needs_backout(spider): # 是否有下一个request，如果没有就break if not self._next_request_from_scheduler(spider): break # 如果还有start_requests 并且没有等待，那么就获取下一个request 并放入队列中 if slot.start_requests and not self._needs_backout(spider): try: request = next(slot.start_requests) except StopIteration: slot.start_requests = None except Exception: slot.start_requests = None logger.error(&apos;Error while obtaining start requests&apos;, exc_info=True, extra=&#123;&apos;spider&apos;: spider&#125;) else: # 将request放进队列 self.crawl(request, spider) if self.spider_is_idle(spider) and slot.close_if_idle: self._spider_idle(spider) _next_request_from_scheduler 主要做的事情 先从scheduler.next_request() 获取队列中的next_request 调用engine._download 下载（需要注意的是，这里经过了下载中间件） 下面会详细分析 下载完成之后调用engine._handle_downloader_output 后面会讲到。下面先讲下如何下载的。1234567891011121314151617181920def _next_request_from_scheduler(self, spider): slot = self.slot request = slot.scheduler.next_request() if not request: return d = self._download(request, spider) ## d.addBoth(self._handle_downloader_output, request, spider) d.addErrback(lambda f: logger.info(&apos;Error while handling downloader output&apos;, exc_info=failure_to_exc_info(f), extra=&#123;&apos;spider&apos;: spider&#125;)) d.addBoth(lambda _: slot.remove_request(request)) d.addErrback(lambda f: logger.info(&apos;Error while removing request from slot&apos;, exc_info=failure_to_exc_info(f), extra=&#123;&apos;spider&apos;: spider&#125;)) d.addBoth(lambda _: slot.nextcall.schedule()) d.addErrback(lambda f: logger.info(&apos;Error while scheduling new request&apos;, exc_info=failure_to_exc_info(f), extra=&#123;&apos;spider&apos;: spider&#125;)) return d engine.crawl 实际就是把request放进调度器重123456789101112def crawl(self, request, spider): assert spider in self.open_spiders, \ &quot;Spider %r not opened when crawling: %s&quot; % (spider.name, request) self.schedule(request, spider) self.slot.nextcall.schedule()def schedule(self, request, spider): self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider) if not self.slot.scheduler.enqueue_request(request): self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider) scheduler.enqueue_request 如果计算指纹 参考看下scrapy.utils.request.py中的request_fingerprint函数12345678910111213def enqueue_request(self, request):# 如果不需要过滤 或者是指纹重复就不进队列 if not request.dont_filter and self.df.request_seen(request): self.df.log(request, self.spider) return False dqok = self._dqpush(request) if dqok: self.stats.inc_value(&apos;scheduler/enqueued/disk&apos;, spider=self.spider) else: self._mqpush(request) self.stats.inc_value(&apos;scheduler/enqueued/memory&apos;, spider=self.spider) self.stats.inc_value(&apos;scheduler/enqueued&apos;, spider=self.spider) return True 上面介绍了如果request如何进队列，下面介绍下scrapy是如何下载request ExecutionEngine._download1234567891011121314151617181920212223def _download(self, request, spider): slot = self.slot slot.add_request(request) def _on_success(response): assert isinstance(response, (Response, Request)) if isinstance(response, Response): response.request = request # tie request to response received logkws = self.logformatter.crawled(request, response, spider) logger.log(*logformatter_adapter(logkws), extra=&#123;&apos;spider&apos;: spider&#125;) self.signals.send_catch_log(signal=signals.response_received, \ response=response, request=request, spider=spider) return response def _on_complete(_): slot.nextcall.schedule() return _ # 这里下载 调用的是scrapy.core.download.Downloader.fetch dwld = self.downloader.fetch(request, spider) # 注册成功之后的调用 dwld.addCallbacks(_on_success) # 注册完成 dwld.addBoth(_on_complete) return dwld scrapy.core.download.Downloader.fetch 注意 这里有个时间是完成之后，会从active中去除掉改request123456789def fetch(self, request, spider): def _deactivate(response): self.active.remove(request) return response self.active.add(request) ## 这里调用的是scrapy.core.downloader.DownloaderMiddlewareManager dfd = self.middleware.download(self._enqueue_request, request, spider) return dfd.addBoth(_deactivate) scrapy.core.downloader.DownloaderMiddlewareManager.download 注意 这个函数主要是调用了下载中间件，最后再proecss_request执行完之后调用** scrapy.core.downloader.Downloader._enqueue_request** 才会真正的调用的下载 这里注册了三个事件, 这些调用定义的下载中间件。 默认的请参考default_setting.py DOWNLOADER_MIDDLEWARES_BASE配置 process_request 处理下载 process_response 处理返回 process_exception 处理异常 process_request 处理完之后会调用self._enqueue_request函数 这个函数12345678910111213141516171819202122232425262728293031323334353637383940414243def download(self, download_func, request, spider): @defer.inlineCallbacks def process_request(request): for method in self.methods[&apos;process_request&apos;]: response = yield method(request=request, spider=spider) if response is not None and not isinstance(response, (Response, Request)): raise _InvalidOutput(&apos;Middleware %s.process_request must return None, Response or Request, got %s&apos; % \ (six.get_method_self(method).__class__.__name__, response.__class__.__name__)) if response: defer.returnValue(response) defer.returnValue((yield download_func(request=request, spider=spider))) @defer.inlineCallbacks def process_response(response): assert response is not None, &apos;Received None in process_response&apos; if isinstance(response, Request): defer.returnValue(response) for method in self.methods[&apos;process_response&apos;]: response = yield method(request=request, response=response, spider=spider) if not isinstance(response, (Response, Request)): raise _InvalidOutput(&apos;Middleware %s.process_response must return Response or Request, got %s&apos; % \ (six.get_method_self(method).__class__.__name__, type(response))) if isinstance(response, Request): defer.returnValue(response) defer.returnValue(response) @defer.inlineCallbacks def process_exception(_failure): exception = _failure.value for method in self.methods[&apos;process_exception&apos;]: response = yield method(request=request, exception=exception, spider=spider) if response is not None and not isinstance(response, (Response, Request)): raise _InvalidOutput(&apos;Middleware %s.process_exception must return None, Response or Request, got %s&apos; % \ (six.get_method_self(method).__class__.__name__, type(response))) if response: defer.returnValue(response) defer.returnValue(_failure) ## 注册事件 deferred = mustbe_deferred(process_request, request) deferred.addErrback(process_exception) deferred.addCallback(process_response) return deferred scrapy.core.downloader.Downloader._enqueue_request/_process_queue/_download 这里涉及到了三个Downloader的函数 -_enqueue_request 将request 和spider封装成slot 放进队列后，调用_process_queue _process_queue 在判断是否需要延迟之后，调用_download，进行下载 _download 调用self.handlers.download_request(默认的是scrapy.core.downloader.handlers.DownloadHandlers)，这个函数会根据不同的scheme选择不同的handler. 默认的分为 1234567 &apos;data&apos;: &apos;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&apos;, &apos;file&apos;: &apos;scrapy.core.downloader.handlers.file.FileDownloadHandler&apos;, &apos;http&apos;: &apos;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&apos;, &apos;https&apos;: &apos;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&apos;, &apos;s3&apos;: &apos;scrapy.core.downloader.handlers.s3.S3DownloadHandler&apos;, &apos;ftp&apos;: &apos;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&apos;,&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def _enqueue_request(self, request, spider): key, slot = self._get_slot(request, spider) request.meta[self.DOWNLOAD_SLOT] = key def _deactivate(response): slot.active.remove(request) return response slot.active.add(request) self.signals.send_catch_log(signal=signals.request_reached_downloader, request=request, spider=spider) deferred = defer.Deferred().addBoth(_deactivate) slot.queue.append((request, deferred)) self._process_queue(spider, slot) return deferreddef _process_queue(self, spider, slot): if slot.latercall and slot.latercall.active(): return # Delay queue processing if a download_delay is configured now = time() delay = slot.download_delay() if delay: penalty = delay - now + slot.lastseen if penalty &gt; 0: slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot) return # Process enqueued requests if there are free slots to transfer for this slot while slot.queue and slot.free_transfer_slots() &gt; 0: slot.lastseen = now request, deferred = slot.queue.popleft() dfd = self._download(slot, request, spider) dfd.chainDeferred(deferred) # prevent burst if inter-request delays were configured if delay: self._process_queue(spider, slot) breakdef _download(self, slot, request, spider): # The order is very important for the following deferreds. Do not change! # 1. Create the download deferred dfd = mustbe_deferred(self.handlers.download_request, request, spider) # 2. Notify response_downloaded listeners about the recent download # before querying queue for next request def _downloaded(response): self.signals.send_catch_log(signal=signals.response_downloaded, response=response, request=request, spider=spider) return response dfd.addCallback(_downloaded) # 3. After response arrives, remove the request from transferring # state to free up the transferring slot so it can be used by the # following requests (perhaps those which came from the downloader # middleware itself) slot.transferring.add(request) def finish_transferring(_): slot.transferring.remove(request) self._process_queue(spider, slot) return _ return dfd.addBoth(finish_transferring) 至此，下载以及下载中间件的处理已经完成了。我们现在回到### _next_request_from_scheduler,看下前面所讲到的engine._handle_downloader_output。 ExecutionEngine._handle_downloader_output 如果返回的response是request 那么久调用crawl方法，将request放入scheduler 如果返回的response 不是request。调用scraper.enqueue_scrape方法，处理response.123456789101112def _handle_downloader_output(self, response, request, spider): assert isinstance(response, (Request, Response, Failure)), response # downloader middleware can return requests (for example, redirects) if isinstance(response, Request): self.crawl(response, spider) return # response is a Response or Failure d = self.scraper.enqueue_scrape(response, request, spider) d.addErrback(lambda f: logger.error(&apos;Error while enqueuing downloader output&apos;, exc_info=failure_to_exc_info(f), extra=&#123;&apos;spider&apos;: spider&#125;)) return d scrapy.core.scraper.Scraper.enqueue_scrape 注册结束处理事件(从active队列中移除掉)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 将结果放入slot队列中def enqueue_scrape(self, response, request, spider): slot = self.slot dfd = slot.add_response_request(response, request) # 这个就是处理完成之后，已出队列中response def finish_scraping(_): slot.finish_response(response, request) self._check_if_closing(spider, slot) self._scrape_next(spider, slot) return _ dfd.addBoth(finish_scraping) dfd.addErrback( lambda f: logger.error(&apos;Scraper bug processing %(request)s&apos;, &#123;&apos;request&apos;: request&#125;, exc_info=failure_to_exc_info(f), extra=&#123;&apos;spider&apos;: spider&#125;)) self._scrape_next(spider, slot) return dfd# 取出队列中的元素，进行处理def _scrape_next(self, spider, slot): while slot.queue: response, request, deferred = slot.next_response_request_deferred() self._scrape(response, request, spider).chainDeferred(deferred)# 注册 两个时间def _scrape(self, response, request, spider): &quot;&quot;&quot;Handle the downloaded response or failure through the spider callback/errback&quot;&quot;&quot; assert isinstance(response, (Response, Failure)) dfd = self._scrape2(response, request, spider) # returns spiders processed output # 这个是处理错误 dfd.addErrback(self.handle_spider_error, request, response, spider) # 处理callback dfd.addCallback(self.handle_spider_output, request, response, spider) return dfd# 如果成功就调用下载中间件以及在经过中间件处理完之后调用call_spider函数，调用，否则就记录下载失败相关信息def _scrape2(self, request_result, request, spider): &quot;&quot;&quot;Handle the different cases of request&apos;s result been a Response or a Failure&quot;&quot;&quot; ## 这里调用的是scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response if not isinstance(request_result, Failure): return self.spidermw.scrape_response( self.call_spider, request_result, request, spider) else: dfd = self.call_spider(request_result, request, spider) return dfd.addErrback( self._log_download_errors, request_result, request, spider)# 调用spider.callback和spider.parse函数def call_spider(self, result, request, spider): result.request = request dfd = defer_result(result) dfd.addCallbacks(request.callback or spider.parse, request.errback) return dfd.addCallback(iterate_spider_output) scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response 先调用process_spider_input, 预处理response 注意此时没有调用spider中的callback或者是parse函数 执行完process_spider_input后 调用scrapy.core.scraper.Scraper.call_spider函数， 如果有异常会调用process_spider_exception 如果无异常会调用 process_spider_output 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def scrape_response(self, scrape_func, response, request, spider): fname = lambda f:&apos;%s.%s&apos; % ( six.get_method_self(f).__class__.__name__, six.get_method_function(f).__name__) def process_spider_input(response): for method in self.methods[&apos;process_spider_input&apos;]: try: result = method(response=response, spider=spider) if result is not None: raise _InvalidOutput(&apos;Middleware &#123;&#125; must return None or raise an exception, got &#123;&#125;&apos; \ .format(fname(method), type(result))) except _InvalidOutput: raise except Exception: return scrape_func(Failure(), request, spider) return scrape_func(response, request, spider) def process_spider_exception(_failure, start_index=0): exception = _failure.value # don&apos;t handle _InvalidOutput exception if isinstance(exception, _InvalidOutput): return _failure method_list = islice(self.methods[&apos;process_spider_exception&apos;], start_index, None) for method_index, method in enumerate(method_list, start=start_index): if method is None: continue result = method(response=response, exception=exception, spider=spider) if _isiterable(result): # stop exception handling by handing control over to the # process_spider_output chain if an iterable has been returned return process_spider_output(result, method_index+1) elif result is None: continue else: raise _InvalidOutput(&apos;Middleware &#123;&#125; must return None or an iterable, got &#123;&#125;&apos; \ .format(fname(method), type(result))) return _failure def process_spider_output(result, start_index=0): # items in this iterable do not need to go through the process_spider_output # chain, they went through it already from the process_spider_exception method recovered = MutableChain() def evaluate_iterable(iterable, index): try: for r in iterable: yield r except Exception as ex: exception_result = process_spider_exception(Failure(ex), index+1) if isinstance(exception_result, Failure): raise recovered.extend(exception_result) method_list = islice(self.methods[&apos;process_spider_output&apos;], start_index, None) for method_index, method in enumerate(method_list, start=start_index): if method is None: continue # the following might fail directly if the output value is not a generator try: result = method(response=response, result=result, spider=spider) except Exception as ex: exception_result = process_spider_exception(Failure(ex), method_index+1) if isinstance(exception_result, Failure): raise return exception_result if _isiterable(result): result = evaluate_iterable(result, method_index) else: raise _InvalidOutput(&apos;Middleware &#123;&#125; must return an iterable, got &#123;&#125;&apos; \ .format(fname(method), type(result))) return chain(result, recovered) dfd = mustbe_deferred(process_spider_input, response) ## 注册了这个callback事件 dfd.addCallbacks(callback=process_spider_output, errback=process_spider_exception) return dfd 结至此，scrapy crawl xxx 整体流程分析完毕 核心类图 其中褐色表示方法 淡黄表示属性 仔细观察还是结合scrapy整体架构图的]]></content>
      <tags>
        <tag>scrapy</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊环境隔离]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%81%8A%E8%81%8A%E7%8E%AF%E5%A2%83%E9%9A%94%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[环境问题是日常开发以及服务上线碰到的问题。这里简单聊下自己所知道的环境隔离的方案 参考有赞环境隔离分享]]></content>
      <tags>
        <tag>环境隔离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊dubbo与zookeeper]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%81%8A%E8%81%8Adubbo%E4%B8%8Ezookeeper%2F</url>
    <content type="text"><![CDATA[dubbo简单来讲就是一个远程调用RPC协议框架。本质上还是个协议 zookeeper是一个注册中心。本质上还是一个软件(software) dubbo 分组 vs zk分组dubbo 分组本意上是当应用有多种版本的时候，可以用group。 （从这点来看用来做环境弱隔离也是可以的）zk分组实际上是根据物理机ip分组。]]></content>
      <tags>
        <tag>java</tag>
        <tag>dubbo</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy(1)深入-环境配置]]></title>
    <url>%2F2019%2F07%2F02%2Fscrapy-1%2F</url>
    <content type="text"><![CDATA[scrapy 是python 爬虫框架中用度最广，且简单的流行款。这里笔者依据源码，分析下 scrapy crawl xxx 之后的操作。 scrapy crawl xxx 调用流程 由setup.py 123entry_points=&#123; &apos;console_scripts&apos;: [&apos;scrapy = scrapy.cmdline:execute&apos;] &#125; 我们找到开始的python 脚本 scrapy.cmdline:execute 主要做了下面几件事情 初始化环境配置，在项目中运行项目 初始化CrawlProcess 调用run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def execute(argv=None, settings=None): if argv is None: argv = sys.argv # --- 兼容之前版本配置 --- if settings is None and &apos;scrapy.conf&apos; in sys.modules: from scrapy import conf if hasattr(conf, &apos;settings&apos;): settings = conf.settings # ------------------------------------------------------------------ # 初始化scrapy.cfg 以及 scrapy.settings.default_setting.py 中的配置 if settings is None: settings = get_project_settings() # set EDITOR from environment if available try: editor = os.environ[&apos;EDITOR&apos;] except KeyError: pass else: settings[&apos;EDITOR&apos;] = editor # 输出deprecated 配置 check_deprecated_settings(settings) # --- 兼容之前版本配置--- import warnings from scrapy.exceptions import ScrapyDeprecationWarning with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;, ScrapyDeprecationWarning) from scrapy import conf conf.settings = settings # ------------------------------------------------------------------ # 判断是否在project中 inproject = inside_project() # 加载支持的命令 即package scrapy.commands下面的所有的文件 集合成字典 cmds = _get_commands_dict(settings, inproject) cmdname = _pop_command_name(argv) parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \ conflict_handler=&apos;resolve&apos;) if not cmdname: _print_commands(settings, inproject) sys.exit(0) elif cmdname not in cmds: _print_unknown_command(settings, cmdname, inproject) sys.exit(2) cmd = cmds[cmdname] parser.usage = &quot;scrapy %s %s&quot; % (cmdname, cmd.syntax()) parser.description = cmd.long_desc() settings.setdict(cmd.default_settings, priority=&apos;command&apos;) cmd.settings = settings cmd.add_options(parser) # 解析参数 opts, args = parser.parse_args(args=argv[1:]) # 这里是对命令中一些参数进行设置 比如是 ```-logfile=xxx.xx```等 _run_print_help(parser, cmd.process_options, args, opts) # 爬虫运行类 cmd.crawler_process = CrawlerProcess(settings) # 执行 cmd 中的run类。 ** 在这里就是执行scrapy.commands.crawler.run函数** _run_print_help(parser, _run_command, cmd, args, opts) sys.exit(cmd.exitcode) scray.crawler.CrawlerProcess 这个类是继承了scray.crawler.CrawlerRunner，主要是增加了一个start函数。需要注意的是init 函数。 scray.crawler.CrawlerRunner.init 中有个属性是spider_loader 是所有爬虫类的加载器，默认使用的是 default_setting.py中的 SPIDER_LOADER_CLASS =’scrapy.spiderloader.SpiderLoader’。 scrapy.commands.crawler.run 这里就是调用了CrawlProcess.crawl/start 函数 123456789101112def run(self, args, opts): if len(args) &lt; 1: raise UsageError() elif len(args) &gt; 1: raise UsageError(&quot;running &apos;scrapy crawl&apos; with more than one spider is no longer supported&quot;) spname = args[0] self.crawler_process.crawl(spname, **opts.spargs) self.crawler_process.start() if self.crawler_process.bootstrap_failed: self.exitcode = 1 CrawlProcess.crawl/CrawlerRunner.crawl 实际上这个调用的是父类 rawlerRunner.crawl 主要工作 初始化并返回一个scrapy.crawler.Crawler实例 调用_crawl函数12345678910def crawl(self, crawler_or_spidercls, *args, **kwargs): if isinstance(crawler_or_spidercls, Spider): raise ValueError( &apos;The crawler_or_spidercls argument cannot be a spider object, &apos; &apos;it must be a spider class (or a Crawler object)&apos;) ## 返回一个scrapy.crawler.Crawler对象 crawler = self.create_crawler(crawler_or_spidercls) ## 调用 CrawlerRunner._crawl 函数 return self._crawl(crawler, *args, **kwargs) scrapy.crawler.Crawler 初始化1234567891011121314151617181920212223242526272829303132333435363738def __init__(self, spidercls, settings=None): if isinstance(spidercls, Spider): raise ValueError( &apos;The spidercls argument must be a class, not an object&apos;) if isinstance(settings, dict) or settings is None: settings = Settings(settings) self.spidercls = spidercls self.settings = settings.copy() self.spidercls.update_settings(self.settings) d = dict(overridden_settings(self.settings)) logger.info(&quot;Overridden settings: %(settings)r&quot;, &#123;&apos;settings&apos;: d&#125;) self.signals = SignalManager(self) self.stats = load_object(self.settings[&apos;STATS_CLASS&apos;])(self) handler = LogCounterHandler(self, level=self.settings.get(&apos;LOG_LEVEL&apos;)) logging.root.addHandler(handler) if get_scrapy_root_handler() is not None: # scrapy root handler already installed: update it with new settings install_scrapy_root_handler(self.settings) # lambda is assigned to Crawler attribute because this way it is not # garbage collected after leaving __init__ scope self.__remove_handler = lambda: logging.root.removeHandler(handler) self.signals.connect(self.__remove_handler, signals.engine_stopped) lf_cls = load_object(self.settings[&apos;LOG_FORMATTER&apos;]) self.logformatter = lf_cls.from_crawler(self) self.extensions = ExtensionManager.from_crawler(self) self.settings.freeze() self.crawling = False # spider 实例 初始化 self.spider = None # 核心engine self.engine = NoneCrawlerRunner._crawl 主要工作 上面初始化了一个Crawler 对象， 调用Crawler.crawl函数（初始化引擎等其他配置） 12345678910111213def _crawl(self, crawler, *args, **kwargs): self.crawlers.add(crawler) // Crawler.crawl d = crawler.crawl(*args, **kwargs) self._active.add(d) def _done(result): self.crawlers.discard(crawler) self._active.discard(d) self.bootstrap_failed |= not getattr(crawler, &apos;spider&apos;, None) return result return d.addBoth(_done) scapy.crawler.Crawler.crawl 主要工作 实例化spider （这个源码就跳过了，就是简单的创建spider实例，并将参数设置好） 实例化engine (下面详细讲下)12345678910111213141516171819202122232425262728293031@defer.inlineCallbacksdef crawl(self, *args, **kwargs): assert not self.crawling, &quot;Crawling already taking place&quot; self.crawling = True try: self.spider = self._create_spider(*args, **kwargs) ## 这个就是实例化了一个scrapy.engine.ExecutionEngine类 self.engine = self._create_engine() start_requests = iter(self.spider.start_requests()) yield self.engine.open_spider(self.spider, start_requests) yield defer.maybeDeferred(self.engine.start) except Exception: # In Python 2 reraising an exception after yield discards # the original traceback (see https://bugs.python.org/issue7563), # so sys.exc_info() workaround is used. # This workaround also works in Python 3, but it is not needed, # and it is slower, so in Python 3 we use native `raise`. if six.PY2: exc_info = sys.exc_info() self.crawling = False if self.engine is not None: yield self.engine.close() if six.PY2: six.reraise(*exc_info) raisedef _create_engine(self): return ExecutionEngine(self, lambda _: self.stop()) scrapy.engine.ExecutionEngine init 函数主要工作 加载调度器(默认是SCHEDULER = ‘scrapy.core.scheduler.Scheduler’) 加载下载器(默认是DOWNLOADER = ‘scrapy.core.downloader.Downloader’) 初始化 scrapy.core.scrapyer.Scraper 对象(下面详解)1234567891011121314151617class ExecutionEngine(object): def __init__(self, crawler, spider_closed_callback): self.crawler = crawler self.settings = crawler.settings self.signals = crawler.signals self.logformatter = crawler.logformatter self.slot = None self.spider = None self.running = False self.paused = False self.scheduler_cls = load_object(self.settings[&apos;SCHEDULER&apos;]) downloader_cls = load_object(self.settings[&apos;DOWNLOADER&apos;]) self.downloader = downloader_cls(crawler) # self.scraper = Scraper(crawler) self._spider_closed_callback = spider_closed_callback scrapy.core.scrapyer.Scraper 主要工作 (连接 middleware pipline spider) 初始化中间件 初始化item1234567891011class Scraper(object): def __init__(self, crawler): self.slot = None self.spidermw = SpiderMiddlewareManager.from_crawler(crawler) itemproc_cls = load_object(crawler.settings[&apos;ITEM_PROCESSOR&apos;]) self.itemproc = itemproc_cls.from_crawler(crawler) self.concurrent_items = crawler.settings.getint(&apos;CONCURRENT_ITEMS&apos;) self.crawler = crawler self.signals = crawler.signals self.logformatter = crawler.logformatter 到此核心插件初始化就完成了。]]></content>
      <tags>
        <tag>scrapy</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之Flink 简单介绍]]></title>
    <url>%2F2019%2F06%2F15%2FFlink-4-What-is-Flink%2F</url>
    <content type="text"><![CDATA[通过前几篇Flink 实战的文章，应该对Flink有点印象了。接下来，本片文章就简单从基本概念，场景， 架构， 特点等方面介绍下Flink. 什么是Flink 官网介绍如下Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. 个人理解Flink 是一个框架和分布式处理引擎，可以对无界或有界数据流进行状态计算。 注意加粗的几个字是核心。下面会聊到。 能够解决什么问题 通用处理流/批处理。 适用场景 实时智能推荐 复杂事件处理 实时数仓与ETL 流数据分析 实时报表分析 基本的概念 流/批处理处理思想 Flink对于批处理认为是有界的流处理。而Spark则认为流处理是更快的批处理。那个更有优势不重要，符合自己的需求并扩展性良好即可。 流 stream 有界流（批处理）vs 无界流（流处理） 有界流：一批连续的数据有开始有结束。（几何中的段的概念） 无界流：连续且无边界的数据流。（几何中射线的概念） 实时流和record Stream 实时流：举例天猫双十一大盘 record Stream： 离线数据清洗。 状态 state 状态可以简单理解为在处理数据的过程中，每个数据的改变都是状态的改变。 时间 time Event time 时间发生的时间 Process Time 消息被计算处理的时间 Ingestion Time 摄取时间：事件进入流处理系统的时间。 架构 组件结构 参考 API&amp;&amp;组件库层 Flink 同时提供了高级的流处理API(eg flatmap filter等),以及相对低级的processFunction. 在API的基础上，抽象除了不同场景下的组件库（FlinkML(机器学习)，CEP(复杂事件处理)等） Runtime层 是Flink的核心层。支持分布式作业的执行，任务调度等。 Deploy 层 部署相关。支持 local, yarn ,cloud等运行环境。 注意 FLink组件库调用API(StreamAPI或者是DataSetAPI)， API(StreamAPI或者是DataSetAPI) 生成jobGraph,并传递给Runtime层，jobGraph 根据不同的部署环境，采用不同的配置项（Flink内置的）执行。 Flink 集群运行时架构 参考 jobManagers(master) 负责 （至少有一个） 协调分布式执行 任务调度 协调检查 协调错误恢复 taskManagers(slave) 负责 （至少有一个） 执行数据流任务 缓存并交换流数据 client 作为数据源的输入，输入之后，可以被清除，也可以或者是处理其他的任务。 特性 流处理 支持高吞吐，低延迟，高性能流处理操作 支持高度灵活窗口(slideWindow SessionWindow等) 支持状态计算，同时具有exactly-once特性 支持 batch on stream API StreamAPI BatchAPI 众多Libraries 机器学习 图处理 。。。 总结Flink 本质上还是只是一个专门解决流批数据处理的框架，并且在性能，稳定，开发，部署等方面具有独到之处。如果我们日常需求中有涉及到大数据处理，且很可能会涉及到协同分布式等，Flink是一个很好的选择。 参考Flink 官网文档]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之收入最高出租车司机]]></title>
    <url>%2F2019%2F06%2F14%2FFlink-learning-in-action-3%2F</url>
    <content type="text"><![CDATA[本篇文章使用纽约市2009-1015年关于出租车驾驶的公共数据集，模拟现实数据流，获取一定时间内收入最高的出租车司机。 输入输出输入: 详见下面数据集输出：每个小时收入收入topN的driverId.额外条件：模拟丢失数据。每n条记录丢失一条数据。 数据集网站New York City Taxi &amp; Limousine Commission提供了关于纽约市从2009-1015年关于出租车驾驶的公共数据集。 下载：12wget http://training.ververica.com/trainingData/nycTaxiRides.gzwget http://training.ververica.com/trainingData/nycTaxiFares.gz TaxiRides 行程信息。每次出行包含两条记录。分别标识为 行程开始start 和 行程结束end。数据集结构 1234567891011rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一id driverId : Long // 出租车司机唯一idisStart : Boolean // 是否是行程开始。false标识行程结束 startTime : DateTime // 行程开始时间endTime : DateTime // 行程结束时间 对于行程开始记录该值为 1970-01-01 00:00:00startLon : Float // 开始行程的经度startLat : Float // 开始行程的纬度endLon : Float // 结束的经度endLat : Float // 结束的纬度passengerCnt : Short // 乘客数量 TaxiRides 数据示例 TaxiFares 费用信息。 与上面行程信息对应 12345678rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一iddriverId : Long // 出租车司机唯一idstartTime : DateTime // 行程开始时间paymentType : String // CSH or CRDtip : Float // tip for this ride (消费)tolls : Float // tolls for this ridetotalFare : Float // total fare collected TaxiFares 数据示例 分析通过上面的数据集以及输入输出的要求，可以分析如下：先根据上满两个数据集，生成输入流。再根据ridrId进行join，对join的结果进行窗口分割，最后对窗口内的数据入库计算收入最高的n个driverId. 思路 生成数据流。（读取上面两个数据集） 模拟丢失数据 filter 根据routId 将两个输入流join (这里其实是过滤掉了filter过滤掉的数据的对应数据) 对上面join的结果划分窗口，并以driverId分组计算窗口内收入，(这里就是简单对taxiFare进行取topN) 选出topN 输出。 部分核心实现新建两个class 表示ride和fare 完整代码 对应source 完整代码 主要逻辑 完整代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public static void main(String[] args) throws Exception &#123; // 初始化enviorment StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置事件事件 environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 读取输入流 DataStreamSource&lt;TaxiFare&gt; taxiFareDataStreamSource = environment .addSource(new TaxiFareSource()); DataStream&lt;TaxiFare&gt; taxiFare = taxiFareDataStreamSource .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getRideId(); &#125; &#125;);// taxiFareDataStreamSource.print(); DataStreamSource&lt;TaxiRide&gt; taxiRideDataStreamSource = environment .addSource(new TaxiRideSource());// taxiRideDataStreamSource.print(); // 对source 过滤掉rided % 1000 == 0, 模拟现实世界丢失数据 DataStream&lt;TaxiRide&gt; taxiRide = taxiRideDataStreamSource.filter(new FilterFunction&lt;TaxiRide&gt;() &#123; @Override public boolean filter(TaxiRide value) throws Exception &#123; return value.isStart &amp;&amp; value.getRideId() % 1000 != 0; &#125; &#125;) .keyBy(new KeySelector&lt;TaxiRide, Long&gt;() &#123; @Override public Long getKey(TaxiRide value) throws Exception &#123; return value.getRideId(); &#125; &#125;); // join 将两个输入流以rideId为key， 合并 SingleOutputStreamOperator&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;&gt; process = taxiFare.connect(taxiRide) .process(new ConnectProcess()); // 设置窗口 SingleOutputStreamOperator&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; aggregate = process // 先将taxiFare 筛选出来，因为是要统计topN taxiFare .flatMap(new FlatMapFunction&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;, TaxiFare&gt;() &#123; @Override public void flatMap(Tuple2&lt;TaxiFare, TaxiRide&gt; value, Collector&lt;TaxiFare&gt; out) throws Exception &#123; out.collect(value.f0); &#125; &#125;) // 因为是时间递增，所以watermark 很简单 .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;TaxiFare&gt;() &#123; @Override public long extractAscendingTimestamp(TaxiFare element) &#123;// System.out.println(element.getEventTime()); return element.getEventTime(); &#125; &#125;) // 根据 driverId 分组 .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getDriverId(); &#125; &#125;) // 设置时间窗口，每30min 计算一次最近1个小时的内driverId的总收入 .timeWindow(Time.hours(1), Time.minutes(30)) // 这个是累加函数,调用aggregate 结果会计算出同一个窗口中，每个driverId的收入总值 .aggregate(getAggregateFunction(), // 这个windowFunc 是格式化输出 new WindowFunction&lt;Float, Tuple3&lt;Long, Float, Timestamp&gt;, Long, TimeWindow&gt;() &#123; @Override public void apply(Long driverId, TimeWindow window, Iterable&lt;Float&gt; input, Collector&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; out) throws Exception &#123; Float next = input.iterator().next(); out.collect(new Tuple3(driverId, next, new Timestamp(window.getEnd()))); &#125; &#125;); // topSize N int topSize = 3; aggregate // 根据时间进行分窗口 .keyBy(2) .timeWindow(Time.hours(1), Time.minutes(30)) .process(new topN(topSize)).print(); environment.execute("RideAndFareExercise"); &#125; 运行结果 参考[ververica]https://training.ververica.com/]]></content>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之获取topNWord]]></title>
    <url>%2F2019%2F06%2F12%2FFlink-learning-in-action-2%2F</url>
    <content type="text"><![CDATA[本片文章基于上一篇文章的逻辑上增加获取topN的逻辑，可以加深对Flink的认识。 描述上一篇文章中只是统计并输出了一定时间内的相同单词的次数，这次我们更深入点，统计一定时间内的前N个word. 思路： 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 增加新窗口（里面的数据是上一步统计好次数之后的单词） 对上面窗口中的数据排序，输出前topN print 输出。 部分代码123456789101112// 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; ret = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 相同字母次数相加 .reduce(CountReduce()) // 滑动窗口，每 30s为一个窗口，每5s计算一次 （新增的逻辑） .windowAll(SlidingProcessingTimeWindows.of(Time.seconds(30), Time.seconds(5))) // 对同一个窗口的所有元素排序取前topSize （新增的逻辑） .process(new TopN(topSize)); TopN class 12345678910111213141516171819202122232425262728293031323334353637383940414243static class TopN extends ProcessAllWindowFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, TimeWindow&gt; &#123; private final int topSize; TopN(int topSize) &#123; this.topSize = topSize; &#125; @Override public void process(Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; elements, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; /* 1 先创建一颗有序树， 2 依次往树里面放数据 3 如果超过topSize 那么就去掉树的最后一个节点 */ TreeMap&lt;Long, Tuple2&lt;String, Long&gt;&gt; treeMap = new TreeMap&lt;&gt;( new Comparator&lt;Long&gt;() &#123; @Override public int compare(Long o1, Long o2) &#123; return o2 &gt; o1 ? 1 : -1; &#125; &#125; ); for (Tuple2&lt;String, Long&gt; element : elements) &#123; treeMap.put(element.f1, element); if (treeMap.size() &gt; this.topSize) &#123; treeMap.pollLastEntry(); &#125; &#125; for (Entry&lt;Long, Tuple2&lt;String, Long&gt;&gt; longTuple2Entry : treeMap.entrySet()) &#123; out.collect(longTuple2Entry.getValue()); &#125; &#125; &#125; 完整代码完整代码请点我]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之第一个Flink程序]]></title>
    <url>%2F2019%2F06%2F11%2FFlink-learning-in-action-1%2F</url>
    <content type="text"><![CDATA[通过本篇文章，帮助你通过使用Maven 快速实现官网demo. 环境 操作系统： mac java版本：1.8 Flink版本：1.7.2 scala版本：2.11 maven 版本:Apache Maven 3.5.2 描述输入为连续的单词，每5s对30s内的单词进行计数并输出。 使用Maven 创建项目12345678mvn archetype:generate \ -DarchetypeGroupId=org.apache.Flink \ -DarchetypeArtifactId=Flink-quickstart-java \ -DarchetypeVersion=1.7.2 \ -DgroupId=Flink-learning-in-action \ -DartifactId=Flink-learning-in-action \ -Dversion=0.1 \ -Dpackage=myFlink 运行成功之后会再当前目录下生成一个名为 Flink-learning-in-action的目录。目录结构 1234567891011$ tree Flink-learning-in-actionFlink-learning-in-action├── pom.xml└── src └── main ├── java │ └── myFlink │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 代码思想 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 print 输出。 部分代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class SocketWindowCount &#123; public static void main(String[] args) throws Exception &#123; ParameterTool parameterTool = ParameterTool.fromArgs(args); String host = parameterTool.get("host", "localhost"); int port = parameterTool.getInt("port", 9000); // 先初始化执行环境 StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并发度为1， 方便观察输出 environment.setParallelism(1); // 输入流 DataStreamSource&lt;String&gt; socketTextStream = environment.socketTextStream(host, port); // 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; reduce = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 计算 .reduce(CountReduce()); // 打印到控制台 输出时间 reduce.addSink(new RichSinkFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void invoke(Tuple2&lt;String, Long&gt; value, Context context) &#123; System.out.println(now() + " word: " + value.f0 + " count: " + value.f1); &#125; &#125;); environment.execute("SocketWindowCount"); &#125; // 统计相同值出现的次数 private static ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; CountReduce() &#123; return new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123; return new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1); &#125; &#125;; &#125; // 将输入的一行 分割成单词，并初始化次数为1 private static FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt; split2Word() &#123; return new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\\W"); for (String word : words) &#123; if (word.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125; &#125; &#125;; &#125;&#125; 运行1 打开终端执行 1nc -l 9000 2 运行代码。 结果: 完整代码完整代码 参考资料java_api_quickstart]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
