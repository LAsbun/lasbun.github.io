<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flink之Flink 简单介绍]]></title>
    <url>%2F2019%2F06%2F15%2FFlink-4-What-is-Flink%2F</url>
    <content type="text"><![CDATA[通过前几篇Flink 实战的文章，应该对Flink有点印象了。接下来，本片文章就简单从基本概念，场景， 架构， 特点等方面介绍下Flink. 什么是Flink 官网介绍如下Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. 个人理解Flink 是一个框架和分布式处理引擎，可以对无界或有界数据流进行状态计算。 注意加粗的几个字是核心。下面会聊到。 能够解决什么问题 通用处理流/批处理。 适用场景 实时智能推荐 复杂事件处理 实时数仓与ETL 流数据分析 实时报表分析 基本的概念 流/批处理处理思想 Flink对于批处理认为是有界的流处理。而Spark则认为流处理是更快的批处理。那个更有优势不重要，符合自己的需求并扩展性良好即可。 流 stream 有界流（批处理）vs 无界流（流处理） 有界流：一批连续的数据有开始有结束。（几何中的段的概念） 无界流：连续且无边界的数据流。（几何中射线的概念） 实时流和record Stream 实时流：举例天猫双十一大盘 record Stream： 离线数据清洗。 状态 state 状态可以简单理解为在处理数据的过程中，每个数据的改变都是状态的改变。 时间 time Event time 时间发生的时间 Process Time 消息被计算处理的时间 Ingestion Time 摄取时间：事件进入流处理系统的时间。 架构 组件结构 参考 API&amp;&amp;组件库层 Flink 同时提供了高级的流处理API(eg flatmap filter等),以及相对低级的processFunction. 在API的基础上，抽象除了不同场景下的组件库（FlinkML(机器学习)，CEP(复杂事件处理)等） Runtime层 是Flink的核心层。支持分布式作业的执行，任务调度等。 Deploy 层 部署相关。支持 local, yarn ,cloud等运行环境。 注意 FLink组件库调用API(StreamAPI或者是DataSetAPI)， API(StreamAPI或者是DataSetAPI) 生成jobGraph,并传递给Runtime层，jobGraph 根据不同的部署环境，采用不同的配置项（Flink内置的）执行。 Flink 集群运行时架构 参考 jobManagers(master) 负责 （至少有一个） 协调分布式执行 任务调度 协调检查 协调错误恢复 taskManagers(slave) 负责 （至少有一个） 执行数据流任务 缓存并交换流数据 client 作为数据源的输入，输入之后，可以被清除，也可以或者是处理其他的任务。 特性 流处理 支持高吞吐，低延迟，高性能流处理操作 支持高度灵活窗口(slideWindow SessionWindow等) 支持状态计算，同时具有exactly-once特性 支持 batch on stream API StreamAPI BatchAPI 众多Libraries 机器学习 图处理 。。。 总结Flink 本质上还是只是一个专门解决流批数据处理的框架，并且在性能，稳定，开发，部署等方面具有独到之处。如果我们日常需求中有涉及到大数据处理，且很可能会涉及到协同分布式等，Flink是一个很好的选择。 参考Flink 官网文档]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之收入最高出租车司机]]></title>
    <url>%2F2019%2F06%2F14%2FFlink-learning-in-action-3%2F</url>
    <content type="text"><![CDATA[本篇文章使用纽约市2009-1015年关于出租车驾驶的公共数据集，模拟现实数据流，获取一定时间内收入最高的出租车司机。 输入输出输入: 详见下面数据集输出：每个小时收入收入topN的driverId.额外条件：模拟丢失数据。每n条记录丢失一条数据。 数据集网站New York City Taxi &amp; Limousine Commission提供了关于纽约市从2009-1015年关于出租车驾驶的公共数据集。 下载：12wget http://training.ververica.com/trainingData/nycTaxiRides.gzwget http://training.ververica.com/trainingData/nycTaxiFares.gz TaxiRides 行程信息。每次出行包含两条记录。分别标识为 行程开始start 和 行程结束end。数据集结构 1234567891011rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一id driverId : Long // 出租车司机唯一idisStart : Boolean // 是否是行程开始。false标识行程结束 startTime : DateTime // 行程开始时间endTime : DateTime // 行程结束时间 对于行程开始记录该值为 1970-01-01 00:00:00startLon : Float // 开始行程的经度startLat : Float // 开始行程的纬度endLon : Float // 结束的经度endLat : Float // 结束的纬度passengerCnt : Short // 乘客数量 TaxiRides 数据示例 TaxiFares 费用信息。 与上面行程信息对应 12345678rideId : Long // 唯一行程idtaxiId : Long // 出租车唯一iddriverId : Long // 出租车司机唯一idstartTime : DateTime // 行程开始时间paymentType : String // CSH or CRDtip : Float // tip for this ride (消费)tolls : Float // tolls for this ridetotalFare : Float // total fare collected TaxiFares 数据示例 分析通过上面的数据集以及输入输出的要求，可以分析如下：先根据上满两个数据集，生成输入流。再根据ridrId进行join，对join的结果进行窗口分割，最后对窗口内的数据入库计算收入最高的n个driverId. 思路 生成数据流。（读取上面两个数据集） 模拟丢失数据 filter 根据routId 将两个输入流join (这里其实是过滤掉了filter过滤掉的数据的对应数据) 对上面join的结果划分窗口，并以driverId分组计算窗口内收入，(这里就是简单对taxiFare进行取topN) 选出topN 输出。 部分核心实现新建两个class 表示ride和fare 完整代码 对应source 完整代码 主要逻辑 完整代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public static void main(String[] args) throws Exception &#123; // 初始化enviorment StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置事件事件 environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 读取输入流 DataStreamSource&lt;TaxiFare&gt; taxiFareDataStreamSource = environment .addSource(new TaxiFareSource()); DataStream&lt;TaxiFare&gt; taxiFare = taxiFareDataStreamSource .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getRideId(); &#125; &#125;);// taxiFareDataStreamSource.print(); DataStreamSource&lt;TaxiRide&gt; taxiRideDataStreamSource = environment .addSource(new TaxiRideSource());// taxiRideDataStreamSource.print(); // 对source 过滤掉rided % 1000 == 0, 模拟现实世界丢失数据 DataStream&lt;TaxiRide&gt; taxiRide = taxiRideDataStreamSource.filter(new FilterFunction&lt;TaxiRide&gt;() &#123; @Override public boolean filter(TaxiRide value) throws Exception &#123; return value.isStart &amp;&amp; value.getRideId() % 1000 != 0; &#125; &#125;) .keyBy(new KeySelector&lt;TaxiRide, Long&gt;() &#123; @Override public Long getKey(TaxiRide value) throws Exception &#123; return value.getRideId(); &#125; &#125;); // join 将两个输入流以rideId为key， 合并 SingleOutputStreamOperator&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;&gt; process = taxiFare.connect(taxiRide) .process(new ConnectProcess()); // 设置窗口 SingleOutputStreamOperator&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; aggregate = process // 先将taxiFare 筛选出来，因为是要统计topN taxiFare .flatMap(new FlatMapFunction&lt;Tuple2&lt;TaxiFare, TaxiRide&gt;, TaxiFare&gt;() &#123; @Override public void flatMap(Tuple2&lt;TaxiFare, TaxiRide&gt; value, Collector&lt;TaxiFare&gt; out) throws Exception &#123; out.collect(value.f0); &#125; &#125;) // 因为是时间递增，所以watermark 很简单 .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;TaxiFare&gt;() &#123; @Override public long extractAscendingTimestamp(TaxiFare element) &#123;// System.out.println(element.getEventTime()); return element.getEventTime(); &#125; &#125;) // 根据 driverId 分组 .keyBy(new KeySelector&lt;TaxiFare, Long&gt;() &#123; @Override public Long getKey(TaxiFare value) throws Exception &#123; return value.getDriverId(); &#125; &#125;) // 设置时间窗口，每30min 计算一次最近1个小时的内driverId的总收入 .timeWindow(Time.hours(1), Time.minutes(30)) // 这个是累加函数,调用aggregate 结果会计算出同一个窗口中，每个driverId的收入总值 .aggregate(getAggregateFunction(), // 这个windowFunc 是格式化输出 new WindowFunction&lt;Float, Tuple3&lt;Long, Float, Timestamp&gt;, Long, TimeWindow&gt;() &#123; @Override public void apply(Long driverId, TimeWindow window, Iterable&lt;Float&gt; input, Collector&lt;Tuple3&lt;Long, Float, Timestamp&gt;&gt; out) throws Exception &#123; Float next = input.iterator().next(); out.collect(new Tuple3(driverId, next, new Timestamp(window.getEnd()))); &#125; &#125;); // topSize N int topSize = 3; aggregate // 根据时间进行分窗口 .keyBy(2) .timeWindow(Time.hours(1), Time.minutes(30)) .process(new topN(topSize)).print(); environment.execute("RideAndFareExercise"); &#125; 运行结果 参考[ververica]https://training.ververica.com/]]></content>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之获取topNWord]]></title>
    <url>%2F2019%2F06%2F12%2FFlink-learning-in-action-2%2F</url>
    <content type="text"><![CDATA[本片文章基于上一篇文章的逻辑上增加获取topN的逻辑，可以加深对Flink的认识。 描述上一篇文章中只是统计并输出了一定时间内的相同单词的次数，这次我们更深入点，统计一定时间内的前N个word. 思路： 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 增加新窗口（里面的数据是上一步统计好次数之后的单词） 对上面窗口中的数据排序，输出前topN print 输出。 部分代码123456789101112// 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; ret = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 相同字母次数相加 .reduce(CountReduce()) // 滑动窗口，每 30s为一个窗口，每5s计算一次 （新增的逻辑） .windowAll(SlidingProcessingTimeWindows.of(Time.seconds(30), Time.seconds(5))) // 对同一个窗口的所有元素排序取前topSize （新增的逻辑） .process(new TopN(topSize)); TopN class 12345678910111213141516171819202122232425262728293031323334353637383940414243static class TopN extends ProcessAllWindowFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, TimeWindow&gt; &#123; private final int topSize; TopN(int topSize) &#123; this.topSize = topSize; &#125; @Override public void process(Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; elements, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; /* 1 先创建一颗有序树， 2 依次往树里面放数据 3 如果超过topSize 那么就去掉树的最后一个节点 */ TreeMap&lt;Long, Tuple2&lt;String, Long&gt;&gt; treeMap = new TreeMap&lt;&gt;( new Comparator&lt;Long&gt;() &#123; @Override public int compare(Long o1, Long o2) &#123; return o2 &gt; o1 ? 1 : -1; &#125; &#125; ); for (Tuple2&lt;String, Long&gt; element : elements) &#123; treeMap.put(element.f1, element); if (treeMap.size() &gt; this.topSize) &#123; treeMap.pollLastEntry(); &#125; &#125; for (Entry&lt;Long, Tuple2&lt;String, Long&gt;&gt; longTuple2Entry : treeMap.entrySet()) &#123; out.collect(longTuple2Entry.getValue()); &#125; &#125; &#125; 完整代码完整代码请点我]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink之第一个Flink程序]]></title>
    <url>%2F2019%2F06%2F11%2FFlink-learning-in-action-1%2F</url>
    <content type="text"><![CDATA[通过本篇文章，帮助你通过使用Maven 快速实现官网demo. 环境 操作系统： mac java版本：1.8 Flink版本：1.7.2 scala版本：2.11 maven 版本:Apache Maven 3.5.2 描述输入为连续的单词，每5s对30s内的单词进行计数并输出。 使用Maven 创建项目12345678mvn archetype:generate \ -DarchetypeGroupId=org.apache.Flink \ -DarchetypeArtifactId=Flink-quickstart-java \ -DarchetypeVersion=1.7.2 \ -DgroupId=Flink-learning-in-action \ -DartifactId=Flink-learning-in-action \ -Dversion=0.1 \ -Dpackage=myFlink 运行成功之后会再当前目录下生成一个名为 Flink-learning-in-action的目录。目录结构 1234567891011$ tree Flink-learning-in-actionFlink-learning-in-action├── pom.xml└── src └── main ├── java │ └── myFlink │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 代码思想 获取运行环境 获取输入源 (socketTextStream) 对输入源进行算子操作 flatMap (拆分成单词，并给个默认值) keyby分组 timeWindow 划分时间窗口 reduce 对每一个窗口计算相同单词出现的次数 print 输出。 部分代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class SocketWindowCount &#123; public static void main(String[] args) throws Exception &#123; ParameterTool parameterTool = ParameterTool.fromArgs(args); String host = parameterTool.get("host", "localhost"); int port = parameterTool.getInt("port", 9000); // 先初始化执行环境 StreamExecutionEnvironment environment = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并发度为1， 方便观察输出 environment.setParallelism(1); // 输入流 DataStreamSource&lt;String&gt; socketTextStream = environment.socketTextStream(host, port); // 对输入的数据进行拆分处理 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; reduce = socketTextStream.flatMap(split2Word()) // 根据tuple2 中的第一个值分组 .keyBy(0) // 设置窗口，每 30s为一个窗口，每5s计算一次 .timeWindow(Time.seconds(30), Time.seconds(5)) // 计算 .reduce(CountReduce()); // 打印到控制台 输出时间 reduce.addSink(new RichSinkFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void invoke(Tuple2&lt;String, Long&gt; value, Context context) &#123; System.out.println(now() + " word: " + value.f0 + " count: " + value.f1); &#125; &#125;); environment.execute("SocketWindowCount"); &#125; // 统计相同值出现的次数 private static ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; CountReduce() &#123; return new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123; return new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1); &#125; &#125;; &#125; // 将输入的一行 分割成单词，并初始化次数为1 private static FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt; split2Word() &#123; return new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\\W"); for (String word : words) &#123; if (word.length() &gt; 0) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125; &#125; &#125;; &#125;&#125; 运行1 打开终端执行 1nc -l 9000 2 运行代码。 结果: 完整代码完整代码 参考资料java_api_quickstart]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
